\chapter{Implementation and Development}

In this chapter, we discuss the main development phase of the project and justify some of the main decisions made throughout its course. We will explore the implementations of the architectures discussed at a high level in Chapter 2 as well as some of the more intriguing problems encountered throughout development.

\figh{figures/trello.png}{A screenshot of the Trello board for the littlegit-core component of the project, taken after the end of development.}{screenshot:trello}{0.25}

\section{Planning and management of work}


Trello boards were used from the very beginning to keep track of project goals and tasks. Separate boards kept track of tasks relating to the server, the littlegit-core and the GUI since they are three distinct codebases. Figure \ref{screenshot:trello} shows the current state of the board for the littlegit-core (though the screenshot cuts off many ``done" tasks). The strategy used with regards to the boards was as follows. Very early in the project, a large number of tasks were created in each board and placed in the \emph{Backlog} column. Every week a few of the highest priority tasks were moved to the next column along, the \emph{Next Priorty} column for completion that week. 


The strategy was first to complete a bulk of the tasks for the littlegit-core, then the server. Once both were stable, we then began work on the GUI. The approach worked reasonably well but did lead to a small amount of thoroughly tested code not being used in the final project.



\section{Choice of language}

Considering the time constraints on this project, only implementing the littlegit-core once, was crucial. Furthermore, the time constraints would not permit writing further wrappers around the littlegit-core to make it compatible with other technologies. Hence the GUI and server needed to be able to import the littlegit-core directly, meaning they needed to use the same language. Three main technologies were considered. They are as follows.

\begin{enumerate}
\item \textbf{JavaScript:} Though this technology has been a staple of web development for many years, in recent years the Electron \cite{electron} has become a popular technology allowing JavaScript, HTML and CSS (traditionally web technologies) to be used to produce cross-platform desktop applications.

Furthermore, NodeJS \cite{nodejs} is a technology allowing JavaScript's use in writing server code, making it a very viable choice as a core technology for littlegit. 

However, JavaScript is a \emph{weakly typed} language, and previous personal experience has indicated that maintaining a large body of weakly typed code quickly becomes very difficult. Despite the advent of tools such as \emph{TypeScript} which is a typed language which transpiles into JavaScript, it is still no replacement for a truly typed language such as Java.

\item \textbf{C\# and .NET:} There exists rich support for developing web server applications using the ASP \cite{asp} framework in C\#, and the project very nearly was written entirely in C\#, a modern, feature-rich language.

However, early experimentation developing a toy C\# application in an OSX environment (used for development) proved difficult and the tooling clumsy. 

\item \textbf{Kotlin:} In the end, the project was written entirely in the Kotlin programming language. Kotlin is a strongly typed language which is fully compatible with Java frameworks and libraries enabling longstanding, reliable frameworks to be used. 

Kotlin is a relatively new language, and though it is inheritance model and general structure is similar to Java (to allow compatibility), it is a very different language to work with. Its rich support for functional programming was heavily utilised primarily in the development of the littlegit-core in the parsing of raw git output. Furthermore, Kotlin is far less verbose than Java allowing much faster development with less boilerplate code to maintain.

Moreover, the Tornado Fx \cite{tornadofx} framework is a relatively new, open source framework which allows the development of cross-platform (Linux, Mac and Windows) desktop applications in Kotlin. 


\end{enumerate}


\section{The littlegit-core}

As has already been discussed, this is the library that interacts directly with the Git binary. The primary challenge here was ensuring we use Git-plumbing commands since these are designed for machine use. A further discussion of Git-plumbing is included in Appendix \ref{appendix_plumbing}.

The project aims to follow modern best practices for software development. For this reason, a TDD (test driven development) approach was used throughout. To achieve this, the library was \emph{designed for testibility}. To aid testability, the library's operations were made synchronous.  Methods which return synchronously prove to be much simpler and more reliable to test, allowing the entire library to be covered thoroughly with tests.

\subsection{Architecture}\label{sec:core-architecture}

The GUI uses the littlegit-core to execute Git commands on local Git repositories. However, the main server needs to be able to execute commands on remote servers over an SSH connection. This requirement is a primary driver behind the architecture of the littlegit-core which visualised in Figure \ref{fig:core-architecture}. 

The top layer is used to combine plumbing commands; for example, multiple plumbing commands are required to change branches. It makes calls to the \texttt{command runner} which provides a high-level interface to these Git commands. The  \texttt{command runner} then converts these high-level invocations into raw commands to be interpreted by Git, which are executed either through the local or remote runners depending on the configuration. Local for the GUI, remote for the server.

\figh{figures/core-architecture}{The internal architecture of the littlegit-core.}{fig:core-architecture}{0.6}


\subsection{Deployment}

We keep mentioning that the library is \emph{used} or \emph{included} by the GUI and server.  Figure \ref{fig:core-deployment} shows the deployment pipeline which concludes by deploying the library to a Maven \cite{maven} repository allowing it to easily be included as a Maven dependency by the server and the GUI, without the need to manually copy any files. The deployment is done using the Travis \cite{travis} continuous integration service (which is also used to deploy and run tests for the other project components).

\figh{figures/core-deployment.pdf}{The deployment pipeline of the littlegit-core.}{fig:core-deployment}{0.5}

\section{The main server}

In this section, we will explore the implementation of the main server and justify the technologies used.

\subsection{Technologies}\label{sec:technologies}

Firstly, Amazon Web Services hosts all the online infrastructure for this project. The justification for this is simple, hosting of databases and servers is expensive, and for students, AWS offers a free tier of the services needed. We will now explore the different technologies considered for components that make up the server.


\subsubsection{Web server framework}

A web server framework was needed as a starting point to build the API. The two frameworks most seriously considered are as follows.

\begin{itemize}
\item The Spring framework \cite{spring}.
\item The Jersey framework \cite{jersey}.
\end{itemize}

We chose Jersey for its lightweight nature, it provides an easy way of creating API endpoints but unlike Spring does not attempt to control the entire stack down to the database allowing much more flexibility. Considering the need for the server to communicate via SSH with the Git servers, having full control of the stack was vital.

The downside of Jersey and the advantage of Spring is a direct consequence. Spring provides much of the required functionality (communication with the database and authentication of users, being most significant) as standard, while these must be implemented manually with Jersey. In the end, the need for flexibility made Jersey the correct choice, despite the additional implementation work involved.

\subsubsection{Database and caching}

Given the rise of non-relational databases such as CouchDB \cite{couchdb} and Firebase \cite{firebase}, the decision to use a traditional SQL database is no longer a trivial one. These non-relational databases have some significant advantages, one of the most enticing being their quality of scaling horizontally, unlike traditional databases which generally only scale vertically. 

However, the decision was made to use a traditional database for the following reasons. The first and most important of these reasons are the need for complex querying on the data where non-relational databases do not compete with the power of traditional SQL. An example of this kind of complex query is the need to find all the Git servers which contain repositories the user either owns or has access to. Moreover, the data has an evident structure, a structure which does not need to change often or adapt to a rapidly changing environment (where non-relational databases come into their own). Because of this, the static, rarely changing schemes of traditional SQL databases are not a hindrance. 

The scaling issue, however, must be addressed. No matter the convenience to the programmer, the use of a technology that will not scale efficiently cannot be justified when one of the objectives of the project is to produce a scalable system. Let us consider the data that the system needs to store. For the reader with experience with relational databases, Figure \ref{fig:entity-relationship} shows an entity relationship diagram for the database. In short, the data we need to store is as follows.

\begin{itemize}
\item User information including tokens used for authorisation and authentication.
\item Information about repositories and the servers upon which they are stored.
\item The SSH keys for each user.
\item Information regarding which users have access to which repositories.
\end{itemize}

\fig{figures/er-diagram.png}{An entity relationship diagram representing the database.}{fig:entity-relationship}{0.5}

The scalability of the server relies on one key observation, the fact that the reading from the database is a far more common operation than writing. For example, the creation of repositories is not a common operation but checking if a user has access to a repository is very common. This observation allows us to introduce caching to solve our scalability issue. By caching as many of the values as possible in a scalable, \textbf{in-memory} caching solution, we massively reduce traffic to the database.

Redis \cite{redis} was selected as the caching solution, its feature set is vast, but the important ones for us are its in-memory key-value store and built-in replication. Its in-memory nature allows for very fast reads of the data letting the cache speed up common simple operations such as returning the details for a particular user, or checking if an authentication token is valid.  Moreover, having a cache with built-in replication allows it to scale across multiple machines easily giving us the horizontal scaling we require. The reader may wonder why we are using a separate caching solution rather than making use of data structures within the server code itself to achieve the same thing.

Let us consider the situation where the load on the server becomes large, causing requests to become slow and even to time out. Since the memory of the machine hosting the server caches the data, the only option is to get a bigger machine with more memory (vertical scaling). On the other hand, using our approach we can spin up multiple machines hosting the server to balance the load, and Redis's inbuilt replication allows us to do the same thing with the cache. Figure \ref{fig:loadbalancing} shows the idea \footnote{This was only tested with one machine each for the cache, server and database without a load balancer due to the cost of doing so on Amazon Web Services, the code however fully supports the architecture described.}.

\figh{figures/loadbalancing.pdf}{The overall architecture of the main server and the way it is designed to scale.}{fig:loadbalancing}{0.5}

\subsubsection{Data transfer}

The decision to use the JSON data format for communication between the server and desktop client was a straightforward one. JSON is lightweight and is easily parsable by a machine while also being straightforward to understand by humans (making debugging easy) \cite{json}.

Furthermore, the decision to create a RESTful interface with the server allows it to be used by other clients in future easily. For example, if a web interface was required to enable users to view and manipulate their remote repositories, the same endpoints could be used without any additional work.

\subsection{Internal server architecture}\label{sec:server-architecture}

While discussing technologies, we saw the overall structure of the server technology stack. In this section, however, we discuss the way the code which interacts with these various components is architected.

As was described in Section \ref{sec:technologies}, a lightweight server framework was selected for the system which does not enforce a particular architecture. The architecture chosen was a \emph{Layered Architecture} as shown in Figure \ref{fig:server-architecture}. Each layer abstracts the one beneath it and interacts exclusively with that layer; this seperation makes testing very straightforward. The roles of the individual layers are described below.

\figh{figures/server-architecture.pdf}{The internal architecture of the main server.}{fig:server-architecture}{0.5}

\subsubsection{Repository Layer}

The repository layer (not to be confused with a Git repository) interacts directly with the database and cache. This involves using raw SQL commands to communicate with the database and is responsible for populating the cache and invalidating cached information when data is updated. By the nature of this layer's direct communication with the SQL database, it is here that considerations must be made to protect against SQL injection attacks.

Furthermore, the repository layer handles the mapping of data from the database and cache to Kotlin data structures. In short, the repository layer is an abstraction on the database and cache, allowing the service layer to work at a level above the raw SQL.


\subsubsection{Service Layer}

In this layer, we have all of our application-specific business logic. For example, the process of creating a new repository consists of several steps, involving multiple writes to the database and the running of commands on remote Git servers. The logic for combining these operations exists in the service layer.

\subsubsection{Controller Layer}

This layer defines the interface between the server's logic and the outside world. This means defining the individual API endpoints and the mappings from HTTP URLs to methods. Importantly, in this layer, we also define what kind of user can access each endpoint. Admin users have access to everything which we mainly use for testing; then we have standard users of the system. The third type of user is the unauthenticated user; we can think of these users as those that are not logged in. Of course, the only endpoints these have access to are those used to login and register new users.

By far, this is the most challenging layer to test since it how we define the interface of our API using syntax provided by the Jersey framework. For this reason, it is vital that the controller contains as little business logic as possible so that the bulk of unit testing can be done against the service layer.

\section{Desktop GUI}

Some of the work involved with building the GUI consisted of standard implementation of user interfaces. Placement of buttons, wiring up of forms e.t.c. In this section, we skate over these tedious parts and also omit discussion of functionality implemented in the GUI already mentioned in other sections.

\subsection{Git graph}

One of the only elements of this project involving the design of an algorithm is the drawing of the so-called \emph{Git graph}, which is a visual representation of the project's history.

Some readers may suppose that given the feature set described in Section \ref{sec:projectgoals}, the resulting history will not be particularly complex since the user is unable to create and merge branches. However, we must consider the situation where an existing (complex) Git repository is opened in littlegit; it is essential that the graph displays correctly. For this reason, the software supports arbitrarily complex Git graphs. Figure \ref{fig:littlegit-graph} shows a project with a reasonably complex history opened in littlegit.

\fig{figures/littlegit-complexgraph.png}{The Git graph for the littlegit-core library opened in littlegit.}{fig:littlegit-graph}{0.2}

Each time the Git history changes (for example when the user creates a commit) the local copy needs to be updated, the graph must be re-drawn. As we saw from our research in Section \ref{sec:teen-design} it is critical that this is as fast as possible. Consider a project with a long history with many thousands of commits. It is not unreasonable that a person from the target demographic may be working with such a project; for example, it may be an exercise to make changes to an open source piece of software. In this case, the graph must update quickly. The key to this is first to calculate which part of the graph is currently visible on the screen based on how far the user has scrolled and the size of the window (a fast calculation). We then only draw the visible portion, rather than drawing everything and letting the window clip out what is out of frame.

Experimentation with the resulting algorithm has shown that the time to redraw the graph is now negligible in comparison with the time taken to read the history from Git in order to draw it (an unavoidable step).

\subsection{Threading}

We have stated repeatedly the importance of software that does not lag considering the target audience. It follows directly that the software should not freeze just because it is performing background tasks. In practice, this means ensuring we do not perform long running tasks on the application's main UI thread. Long-running tasks include API calls (since the network may be slow), interactions with the file-system, including calls to the littlegit-core since invoking some Git commands may be slow. On the face of it, this seems straightforward. Upon needing to run one of these tasks, we run it on a separate thread and wait for it to complete before updating the UI.

Unfortunately, the fact that we are interacting with a Git repository complicates matters. Consider the scenario we make a call to the littlegit-core to create a new commit on a very slow machine, suppose that multiple files have changed and must be included in the commit. Now suppose the user tries to check out a different commit. Checking out a different commit while halfway through the creation of another will lead to Git throwing errors and possibly corrupting the repository. To prevent this, we have two options; whenever any background task runs, we disable the entire UI to prevent the user from doing anything. As we have discussed this is bound to anger our users.

The second option, (the option used by littlegit) is to use a queue instead. All operations which depend on these long-running tasks are added to a queue and executed one by one (on a separate thread). The scenario described above now performs as a user would expect, one operation following the other automatically without the need for repeated clicking.

\section{Authentication}

There are two main kinds of authentication used by the system. SSH authentication is used when the desktop GUI performs Git operations interacting with the remote Git server (for example \texttt{git push} and \texttt{git fetch}), whereas calls to the API use a token-based system. 

\subsection{API authentication}

The token-based system used to authenticate calls to the API implements the OAuth 2.0 protocol since at the time of writing this is the industry standard for authentication \cite{oauth2}. Furthermore, the use of tokens also makes it easy to cache authentication details for a user which helps speed up requests.

\subsection{SSH authentication}

Implementing secure SSH based authentication between the desktop GUI and the Git servers is one of the key technical challenges of the project. Firstly we must explore what happens when we run a command such as \texttt{git fetch}. Internally, Git runs a command using SSH on a remote machine. The context under which the command runs, has a user in the environment of the remote machine, by convention this user is called \emph{git}. 

The first task was to ensure that the user \emph{git} could only execute Git commands since otherwise, it would be possible to execute any shell command on the remote machine, a clear security risk. To achieve this, the \emph{git} user is configured not to use the standard Unix shell, but instead, to use a tool called the \textbf{git-shell} \cite{git-shell} which enforces the restrictions (this is reliable since the creators of Git itself wrote the tool).

On the other hand, when a user creates a new repository, it is the responsibility of the main server to create this repository on one of the Git servers. This requires a different user to the \emph{git} user since it needs permission to create a directory and initialise the repository. Due to this need, it was necessary to carefully set file permissions to ensure both users have only the permissions they need and no more, to reduce the threat of malicious access.

Shell commands are used on the Git servers, invoked by SSH to check with the main server if a specific user action is allowed before completing it. This ensures the main server controls repository access allowing access to be easily revoked from users if needed. Appendix \ref{appendix_gitauth} describes details of the full authentication process.



