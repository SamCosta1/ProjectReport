\chapter{Evaluation and Testing}

This Chapter discusses the approach taken to test and evaluate the system. We'll discuss both the automated testing strategy and the steps taken to procure feedback from likely users. 

The focus of the automated tests was to ensure a stable and reliable product, to catch bugs and help prevent regressions. On the other hand, the primary aim of user testing was not on finding bugs due to the limited amount of time available interacting with likely users (i.e. teenagers). It was decided that obtaining feedback about the usability and usefulness of the system was a better use of this time, rather than having users try to break the system to discover faults.

\section{Automated Testing}

This project consists of three distinct codebases. Both the littlegit-core and desktop GUI are required to work cross-platform, as such, all the automated tests were run in OSX, Windows and Ubuntu-Linux environments. A test-driven approach was used across the board but the testing strategy for each is slightly different, and we will discuss them separately. 

\subsection{Littlegit-core}

Conventional wisdom with regards to automated testing is to follow the \textbf{Cohn test automation pyramid} \cite{testpyramid}. It stipulates that a large proportion of the tests should be unit tests with a smaller proportion of integration and UI tests. For testing the littlegit-core, this guideline was purposely ignored, and the vast majority of the tests are integration tests. These examine the library's interaction with real Git repositories (created and managed by the test case) and ensure results are correct. 

To implement these tests as unit tests, we would have to mock Git's output at each stage of the test. Such an approach quickly proves impractical since minor changes to the test code require new dummy Git output to be generated each time, a slow manual process, resulting in the decision to use integration tests against Git itself. The downside to our chosen approach is that test cases execute marginally more slowly due to the dependency on Git and the filesystem. In practice, however, the suite of over a hundred tests still runs (on average) in under 15 seconds making this a non-issue for development.

Regarding code coverage, as Figure \ref{fig:core-coverage} shows we have code coverage above 90\% which we consider more than acceptable. Furthermore, in using the library in the development of the project's other components, it has proven stable, with only a few bugs discovered.

\fig{figures/core-coverage.png}{The code coverage statistics for the littlegit-core library.}{fig:core-coverage}{0.5}

\subsection{The main server}

The main server called for an entirely different testing strategy to that used for the littlegit-core. The chosen approach follows the conventional wisdom; many unit tests, fewer service level tests (of course no GUI tests since it has no GUI).

Recall the server architecture we discussed in Section \ref{sec:server-architecture}. Many unit tests are used to test the \emph{Repository} layer, the layer that interacts with the database. The decision was made to test against a real database and cache rather than try to mock out the connections since this is tedious. We also get the advantage of checking the database and cache connections using this approach.

To unit test the \emph{Service} layer we mock out the dependency on the \emph{Repository} layer to ensure we only test the Service layer functionality (similarly for the \emph{Controller} layer).  A much smaller number of integration tests are used to check the entire stack from the \emph{Controller} layer down to the database, the aim being to catch any issues in the integration of the various layers. Code coverage for the server is lower than the littlegit-core but still stands at 76\% which is acceptable.

Formal performance tests were not used in the development of the server. Manual testing shows that calls to the API execute quickly and that the caching strategy does indeed speed up data retrieval. Any further insights that formal performance tests would provide would require the system to run on multiple machines as previously described leading to a significant monetary cost which was deemed unnecessary.

\subsubsection{Improvements}

Manual testing was used to sanity check the API endpoints. Involving running the server, and then calling the endpoints with input data and reviewing the JSON response. Automating this process would be useful, running tests after the server has been deployed, sanity checking that all the endpoints are working and that there are no glaring issues.
 
\subsection{Desktop GUI}

By far, this codebase was the most difficult to test owing to the fact most of the operations to be tested run asynchronously. The strategy, therefore, was to concentrate on the code that performs automation of Git operations, ensuring that the system reacts correctly to Git errors and the user mistakes causing them. Furthermore, it was essential to verify the GUI responded correctly to issues with the network connection and errors from the API. Mocking was used to simulate these conditions and ensure the system reacted appropriately.

Unit testing was used extensively to ensure that the desktop GUI's internal data persistence store was stable, tests that proved useful since minor mistakes in the persistence code almost caused multiple serious regressions which were caught by the tests.

It was decided that UI tests would not be used since the Tornado FX framework's support for them is limited, and early experimentation showed them to be unreliable. Due to the lack of UI tests, the test coverage of the GUI is by far the lowest of the components at 33\%. However, if we omit UI code from this figure, our coverage is close to 80\% which we regard as acceptable.

\section{User testing}\label{sec:user-testing}

As was briefly mentioned in Chapter 2, a true iterative approach to developing the GUI was not possible due to a limited number of opportunities to seek feedback from the target audience. 

However, two feedback sessions with teenagers occurred during development, the first roughly halfway through and the second nearing the end. The participants in the sessions were all sixteen years olds studying GCSEs in Computer Science. They had not used any version-control systems in the past but had received a short talk about Git specifically before the feedback session.

\subsubsection{First feedback session}

During the first session, the software was unfinished with a few bugs in the way repositories were created and opened. To mitigate this, a new (empty) repository was opened in the software beforehand. Questions were asked to the group as a whole, while tasks were carried out by a single individual. The questions and tasks along with a summary of responses to them are as follows.\\

\noindent \emph{Question: } What are your general first impressions of the application?

\noindent \emph{Summary of responses:} General opinion was favourable; participants liked the dark colour scheme and the uncluttered interface. One participant commented that the limited amount of text made it much simpler to use by those with dyslexia like themselves. \\

\noindent \emph{Task: } Could you make a change to a sample file and commit those changes?

\noindent \emph{Observations: } It was very encouraging to see the task was completed smoothly without any issues. Even more encouragingly the user then clicked on the commit to examine its details without being prompted to do so. When asked, they commented that it seemed intuitive to use the graph to view details of the commit.\\

\noindent \emph{Task: } Could you create another couple of commits and then go back to the first version?

\noindent \emph{Observations: } Again, this task was carried out smoothly. Performed by a different person to the first task, they added a few commits and took very little time exploring the interface before finding the option to switch back to an old version. Another member of the group who was observing commented that they liked the fact they could use the graph for navigating back and forth.\\

\noindent \emph{Question: } For this question, participants were presented with the interface to GitKraken (discussed in Chapter 2), they were asked for any comments they had comparing the two applications.

\noindent \emph{Summary of responses: } The overwhelming reaction to GitKraken's interface was that it was difficult to navigate and to understand what was going on. Many of the options and buttons (including branching and merging) were explained to participants and most commented that they didn't see such features as being very useful in their own workflows. 

Overall, the consensus was that for their current needs and level of experience that the simplified interface of littlegit was preferable to the more sophisticated interface of what we might consider a more conventional Git GUI. This piece of feedback was particularly useful since it indicates that we did not \emph{over-simplify} Git to the extent it stopped being useful. \\

\noindent \emph{Overall comments: } One of the main takeaways from this first session was that while obtaining the positive feedback was indeed reassuring, for the next session it was necessary to try to push for more criticism. In other words to make participants comfortable in expressing suggestions and pointing out issues they saw in the software.

\subsubsection{Second feedback session}

The format of the first session worked well and was therefore used for the second session. The second group made up of some new faces and some of the same participants as the first group; however, the main difference was that before starting, criticism and negative comments were encouraged. The questions, tasks and observations are as follows.\\

\noindent \emph{Task: } Could you open the sample directory in littlegit?\\
\noindent \emph{Observations: } The user (intentionally chosen to be someone who hadn't been in the previous group and hadn't seen the software before) successfully opened the indicated directory in the software. They were then asked to create a few commits and navigate back and forth through the commit history as in the previous session and had no issues doing so.\\

\noindent \emph{Task: } Could you open one of the existing repositories that only exist on the server?\\
\noindent \emph{Observations: } This task caused some confusion, and it was necessary to explain further what was meant by repositories not yet existing locally. The user was able to open the repository in littlegit but was unable to locate it on the file system; a clear need for improvement in the software. Participants suggested a button to open the directory in the file explorer which would be an excellent solution to the problem. Further work is certainly needed in this area to improve the user experience. \\

\noindent \emph{Overall comments:}
During the session, one of the participants mentioned that the Git graph in the software looked odd when it only had one commit. Others agreed, and it is easy to see why. The commit is highlighted but has no other commit to distinguish it from which looks odd. Figure \ref{fig:odd-commit} shows the software in this state.

\fig{figures/odd-commit.png}{The main interface of littlegit with only one commit.}{fig:odd-commit}{0.25}

Unfortunately, time constraints prevented these concerns from being adequately addressed during the main project lifecycle, but these are indeed areas for future improvement, making the feedback valuable.

\subsubsection{Other user feedback}

Other, less formal feedback sessions were carried out numerous times within a small tutor group involving university students and lecturers where feedback was obtained about the project's progress. Everyone in the room had experience using Git, and therefore the format was slightly different. No tasks were set; instead, the features of the software were briefly demonstrated and feedback gathered. These sessions were instrumental in the initial designs described in Chapter 2, and certain features (the Git graph chief among them) were iterated upon multiple times based on feedback from these sessions.

